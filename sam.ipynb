{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "Running on CPU.\n",
      "Cell 1 time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import time\n",
    "import os\n",
    "\n",
    "global_start_time = time.time()\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"Running on CPU.\")\n",
    "\n",
    "cell_start = time.time()\n",
    "print(f\"Cell 1 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1037\n",
      "Label distribution:\n",
      "toxic            520\n",
      "severe_toxic      80\n",
      "obscene          441\n",
      "threat             1\n",
      "insult           520\n",
      "identity_hate     28\n",
      "dtype: int64\n",
      "Cell 2 time: 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "data_path = \"marathi_hindi_toxicity_dataset.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {os.path.abspath(data_path)}\")\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "texts = df['comment_text'].tolist()\n",
    "labels = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values  # Six labels\n",
    "\n",
    "print(f\"Dataset size: {len(df)}\")\n",
    "print(f\"Label distribution:\\n{df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum()}\")\n",
    "print(f\"Cell 2 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 3 time: 2.03 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "# Define local path for model\n",
    "local_path = \"./distilbert_local\"\n",
    "if not os.path.exists(local_path):\n",
    "    os.makedirs(local_path)\n",
    "    # Download model and tokenizer\n",
    "    try:\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "        model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-multilingual-cased\",\n",
    "            num_labels=6,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        tokenizer.save_pretrained(local_path)\n",
    "        model.save_pretrained(local_path)\n",
    "        print(f\"Downloaded and saved model to {local_path}\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to download model: {e}. Ensure internet connection or use pre-downloaded model.\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(local_path)\n",
    "\n",
    "# Create config for multi-label classification\n",
    "config = DistilBertConfig.from_pretrained(\n",
    "    local_path,\n",
    "    num_labels=6,\n",
    "    problem_type=\"multi_label_classification\",\n",
    "    hidden_dropout_prob=0.3,\n",
    "    attention_probs_dropout_prob=0.3\n",
    ")\n",
    "\n",
    "# Load model with ignore_mismatched_sizes\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    local_path,\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "device = torch.device(\"cpu\")  # Force CPU\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Cell 3 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 829, Validation size: 104, Test size: 104\n",
      "Cell 4 time: 0.34 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "# Split data: 80% train, 10% validation, 10% test\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Encode texts\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "\n",
    "print(f\"Train size: {len(train_texts)}, Validation size: {len(val_texts)}, Test size: {len(test_texts)}\")\n",
    "print(f\"Cell 4 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 5 time: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "class ToxicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].to(device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx].to(device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ToxicDataset(train_encodings, train_labels)\n",
    "val_dataset = ToxicDataset(val_encodings, val_labels)\n",
    "test_dataset = ToxicDataset(test_encodings, test_labels)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=8)\n",
    "\n",
    "print(f\"Cell 5 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 6 time: 5.08 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)\n",
    "\n",
    "print(f\"Cell 6 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: Train Loss: 0.2810, Val Loss: 0.0920, Val Accuracy: 0.9615\n",
      "Epoch 2/3: Train Loss: 0.0668, Val Loss: 0.0306, Val Accuracy: 1.0000\n",
      "Epoch 3/3: Train Loss: 0.0285, Val Loss: 0.0148, Val Accuracy: 1.0000\n",
      "Training completed.\n",
      "Cell 7 time: 444.60 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "def train_model(epochs=3, patience=2):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "            labels = batch['labels']\n",
    "            outputs = model(**inputs).logits\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_preds, val_true = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "                labels = batch['labels']\n",
    "                outputs = model(**inputs).logits\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"Marathi_hindi_best_model.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "train_model(epochs=3, patience=2)\n",
    "print(f\"Training completed.\")\n",
    "print(f\"Cell 7 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Precision: 1.0000, Recall: 1.0000, Accuracy: 1.0000\n",
      "Cell 8 time: 3.37 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"Marathi_hindi_best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {key: val for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels']\n",
    "        outputs = model(**inputs).logits\n",
    "        preds = torch.sigmoid(outputs) > 0.5\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "precision = precision_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average='micro', zero_division=0)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Test Precision: {precision:.4f}, Recall: {recall:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(f\"Cell 8 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring example comment:\n",
      "toxic: 0.7244\n",
      "severe_toxic: 0.4342\n",
      "obscene: 0.7337\n",
      "threat: 0.3156\n",
      "insult: 0.7361\n",
      "identity_hate: 0.3365\n",
      "Cell 9 time: 0.16 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "def score_comment(comment):\n",
    "    inputs = tokenizer(comment, truncation=True, padding=True, max_length=128, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    probs = torch.sigmoid(outputs).cpu().numpy()[0]\n",
    "    categories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    return \"\\n\".join(f\"{cat}: {prob:.4f}\" for cat, prob in zip(categories, probs))\n",
    "\n",
    "example_comment = \"are murkha\"\n",
    "print(\"\\nScoring example comment:\")\n",
    "print(score_comment(example_comment))\n",
    "\n",
    "print(f\"Cell 9 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total execution time: 8.62 minutes\n",
      "Cell 10 time: 0.57 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start = time.time()\n",
    "\n",
    "model.save_pretrained(\"./marathi_hindi_toxic_detector_model\")\n",
    "tokenizer.save_pretrained(\"./marathi_hindi_toxic_detector_model\")\n",
    "print(f\"\\nTotal execution time: {(time.time() - global_start_time) / 60:.2f} minutes\")\n",
    "\n",
    "print(f\"Cell 10 time: {time.time() - cell_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
